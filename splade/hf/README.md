# SPLADE - HuggingFace training

**TL; DR** We provide a new code version to train SPLADE models based on HuggingFace trainers. Compared to the original code base, it allows training SPLADE with several *hard* negatives, training with Distributed Data Parallel etc., making the overall training process more effective and efficient. 
It also differs in various aspects -- for instance, we remove the scheduler for the regularization hyperparameters, add the "anti-zero" trick to avoid representations collapsing to zero vectors etc. 

This code is solely meant to **train** models. To index and retrieve with SPLADE, everything remains the same.

## Data format

To train models, four files are needed (used in the `src/hf/datasets.py` file):

* **collection file** : *tsv* file, `ID\tDATA`, contains the (text) documents
* **query file** : *tsv* file, `ID\tDATA`, contains the (text) queries
* **qrel file** : *json* file, `{QID: {DID_1: rel_1, DID_2: rel_2, ...}, ...}`, ground-truth relevance
* **score (or hard-negative file)** : here, we allow for several formats
    * **saved_pkl** (resp. **pkl_dict**) : pickle file (resp. gzip), *json* format, `{QID: {DID_1: score_1, DID_2: score_2, ...}, ...}` (the latter having the format of `cross-encoder-ms-marco-MiniLM-L-6-v2-scores.pkl.gz` from this [page](https://huggingface.co/datasets/sentence-transformers/msmarco-hard-negatives/tree/main), where the file is compressed and the ids are integers)
    * **trec** : trec format file (e.g., generated by Anserini)
    * **json** : dict result from a run, *json* format ; for instance to train a SPLADE model with SPLADE negatives (from a first round of training)

Note that training w/o distillation still relies on a "score" file (but the scores are not used).

## Getting started

To keep full backward compatibility with the original SPLADE code, we allow training models with [Hydra](https://hydra.cc/) configurations. The mapping between hyperparameters is done in `src/hf/convertl2i2hf.py`.

To specify (HF) arguments in the command line (for instance the number of negatives), please refer to `src/SLURM/hf.sh`.

In particular, training can be launched with :

```
model=distilbert-base-uncased
config=config_hf_splade_distill_l1q.yaml
python -m torch.distributed.launch --use_env --nproc_per_node 4  --master_port $port -m splaed.hf_train  --config-name=$config \
                     config.lr=2.0e-5 \
                     +data.flops_queries=/path/to/flops/queries \
                     config.nb_iterations=1000 \
                     config.checkpoint_dir=$dir/chk/  \
                     config.index_dir=$dir/index/  \
                     config.out_dir=$dir/out/  \
                     init_dict.model_type_or_dir=$model \
                     config.tokenizer_type=$model \
                     config.record_frequency=100000\
                     config.train_monitoring_freq=100000 \
                     config.regularizer.FLOPS.lambda_d=0.0005 \
                     config.regularizer.L1.lambda_q=.001 \
                     config.train_batch_size=4 \
                     config.max_length=128 \
                     config.warmup_steps=1 \
                     +hf.training.logging_dir=$dir/chk/ \
                     +hf.training.num_train_epochs=5 \
                     +hf.training.warmup_ratio=0.01 \
                     +hf.training.weight_decay=0 \
                     hf.model.dense=false \
                     +hf.data.distillation=true \
                     hf.data.n_negatives=32 \
                     +hf.model.shared_weights=false \
                     +hf.data.scores=/path/to/scores
```

where for instance `+hf.data.scores` indicates the path to a score file (all the HF hp are prefixed wih `hf`).

After training, indexing and retrieval can be launched with :

```

python   -m src.index  --config-path=$dir/chk   --config--name=config

python   -m src.retrieve  -config-path=$dir/chk   --config--name=config

```

## Example

The config file `conf/config_f_distilbert.yaml` corresponds to  a training with toy data

